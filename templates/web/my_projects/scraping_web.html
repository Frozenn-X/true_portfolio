{% extends 'base.html' %}
{% load static %}
{% block title %}Scraping Web - Portfolio Xavier{% endblock %}
{% block favicon %}<link rel="shortcut icon" type="image/png" href="{% static 'img/photo_lkd.jpg' %}"/> {% endblock favicon %}
{% block content %}
    <div class="container row mx-auto h-100 my-lg-0 my-5 pb-5" style="padding-top: 100px;">
        <h1 class="text-center my-5 text-info">Scraping web - Google Business Profile</h1>
        <p>
            L'un de mes premiers projets chez Avenir Data, a été de récupèrer des données sur internet pour faire des
            statistiques et évaluer le marché et les positions sur les fiches Google Business Profil en utilisant Python
            et la libraire Selenium.
        </p>
        <div class="row">
            <div class="col-lg-8 col-12 my-auto">
                <div class="row my-2">
                    <h4 class="text-info">Les objectifs :</h4>
                    <p>
                        Récupérer les données des <a class="text-info" href="https://www.google.com/intl/fr_fr/business/" target="_blank"><u>fiches Google Business Profile</u><i class="fa-solid fa-arrow-up-right-from-square"></i></a> ,
                        regrouper ces données afin d'augmenter leurs valeurs, faire des statistiques sur le classement des fiches googles.
                    </p>
                </div>
                <div class="row my-2">
                    <h4 class="text-info">Le contexte :</h4>
                    <p>
                        Notre travail dans le scraping et de récupèrer des données sans violé la RGPD, et pour créer des statistiques utilisable afin de comprendre aux mieux l'algorithme Google,
                        en analysant les données de centaines de Google local pack, nous pouvions mieux comprendre les problèmatiques pour s'améliorer et les éléments que Google mets en avant
                        afin de permettre à nos clients de se positionner sur un nouveau marché, ou se placer
                        dans le top 3 du leur. Aujourd'hui l'algorithme des fiches Google et plus précisément du
                        <a href="https://www.linkedin.com/posts/datactive40_google-local-pack-activity-6990208845226594304-XTRK?utm_source=share&utm_medium=member_desktop" class="text-info" target="_blank">
                        Google Local Pack <i class="fa-solid fa-arrow-up-right-from-square"></i></a> sont peux connus, étudier celui-ci en récupérant des informations pouvais
                        donnez un avantage à notre outil.
                        Chaque client était aussi une source d'aide et d'amélioration de notre compréhension de l'algorithme Google.
                    </p>
                </div>   
                <div class="row my-2">
                    <h4 class="text-info">L'enjeu :</h4>
                    <p>
                        Dans ce projet, il était important de bien récupérer les données correctes afin de pouvoir créer des
                        statistiques et mieux décrypter l'algorithme de oogle afin de pouvoir aidez nos clients à mieux se positionner.
                    </p>
                </div>      
            </div>
            <div class="col-lg-4 col-12 text-center m-auto py-3">
                <img src="{% static 'img/Google_local_pack.png' %}" class="w-100"/>
                <em>Image d'une page Google à scraper grâce à Selenium</em>
            </div>
        </div> 
        <div class="my-2">
            <h4 class="text-info">Les risques :</h4>
            <p>
                L'un des plus gros risques dans ce projet, était l'échec de se retrouver bloqué lors de la récupération des données, par le captcha Google,
                qui aime ses pages à chaque nouvelle mise à jour afin de justement éviter ce genre intensif ou même dans les cas de se voir son IP bloqué.
            </p>
        </div>
        <div class="my-2">
            <h4 class="text-info">Les étapes :</h4>
            <p>
                La première étape consistait à trouver le moyen de pouvoir récupérer de manière fiable ces informations,
                sur les pages du <a href="https://www.linkedin.com/posts/datactive40_google-local-pack-activity-6990208845226594304-XTRK?utm_source=share&utm_medium=member_desktop" class="text-info" target="_blank">
                Google Local Pack <i class="fa-solid fa-arrow-up-right-from-square"></i></a> .
            </p>
            <p>
                Une fois cela fait nous pouvions voir pour quels mots-clés Google nous renvoyait des <a href="https://www.linkedin.com/posts/datactive40_google-local-pack-activity-6990208845226594304-XTRK?utm_source=share&utm_medium=member_desktop" class="text-info" target="_blank">
                Google Local Pack <i class="fa-solid fa-arrow-up-right-from-square"></i></a> , et les classement des fiches selon une catégorie ou un secteur.
            </p>
            <p>
                En parallèle, nous devions vérifier les données et trouver le moyen d'accélérer le processus, pour lequel nous avons établi des stratégies,
                comme multiplier le programme en le déployant avec un docker, containerisant le programme, mais aussi les blocages par le navigateur.
                Chaque docker avait un système afin de se relancer tout seul, modifiant l'user-agent et le VPN utilisé pour éviter les blocages d'IP de Google.
            </p>
            <p>
                Ensuite, une fois cela fait, nous pouvions créer des statistique pour vérifier si par exemple, il y avait une corrélation entre une ou plusieurs informations présentes dans les fiches Google et l'ordre dans laquelle une fiche ressortait selon la recherche.
            </p>
            <p>
                Il en est ressortis que plusieurs des premières analyses faites ont pu être utiles à nos clients et leur
                ont rapporté, afin d'optimiser cela j'ai pu me rendre utiles en optimisant les recherches et l'analyse
                des données récupèrer en implémentant un processus de multi-threading & changez certains mots-clés pour un
                secteur précis afin de voir si cela impacté le classement des fiches <a href="https://www.google.com/intl/fr_fr/business/" class="text-info" target="_blank">
                Google Business Profile <i class="fa-solid fa-arrow-up-right-from-square"></i></a> 
                Un système de points selon la complétion d'une fiche, et les horaires de scraping nous permettaient aussi d'en savoir plus sur l'algorithme de Google.
            </p>
            <p>
                J'ai adoré la complétion des tâches entre la récupération de données massives, et l'analyse faites derrière, pour essayer de comprendre comment l'améliorer.
Cela notamment dans le fait qu'on peut voir les différences entre plusieurs moments de la journée, et la multitude de possibilités qu'il pouvait y avoir.

Mon niveau sur le scraping, aujourd'hui, est intermédiaire  Je sais facilement utiliser les différentes librairies, grâce notamment aux Xpath qui est plus ou moins similaire selon le langage de balise qu'on veut récupérer ou parser.
Pour moi, je trouve l'outil Selenium plus au point que ses compères, notamment dus au fait qu'il puisse directement simuler un navigateur, même sans interface et tout de même agir sur le JavaScript ce qui implique pas mal de chose, comme le fait de pouvoir simulé un vrai comportement humain, tout en évitant une charge trop lourde.
            </p>
        </div>
        <div class="col-lg-8 col-12 mt-5">
            <h5>Les compétences utilisés dans ce projet sont :</h5>
            <ul class="px-5">
                <li>
                    <a href="{% url 'web:bdd_relationnal-desc' %}" class="text-info" target="_blank">Base de données relationnelles (Stockage & mise en relations des données)</a>
                </li>
                <li>
                    <a href="{% url 'web:comm-desc' %}" class="text-info" target="_blank">Communication (Comprendre le besoin du client)</a>
                </li>
                <li>
                    <a href="{% url 'web:sql-desc' %}" class="text-info" target="_blank">SQL (Manipulation & analyse des données)</a>
                </li>
                <li>
                    <a href="{% url 'web:python-desc' %}" class="text-info" target="_blank">Python (Script de crawling et scraping web)</a>
                </li>
            </ul>
        </div>
        <div class="col-lg-4 col-12 mt-5">
            <h5>Technologie associés :</h5>
            <ul class="px-5">
                <li>Scrapy</li>
                <li>
                    <a href="https://www.docker.com/" target="_blank" class="text-info">Docker <i class="fa-solid fa-arrow-up-right-from-square"></i></a></li>
                <li>
                    <a href="https://pypi.org/project/selenium/" target="_blank"  class="text-info">Selenium (librairie permettant la manipulation d'un navigateur) <i class="fa-solid fa-arrow-up-right-from-square"></i></a>
                </li>
                <li><a href="https://mariadb.org/" target="_blank" class="text-info">MariaDb <i class="fa-solid fa-arrow-up-right-from-square"></i></a></li>
                <li>Git</li>
            </ul>
        </div>
        <h5 class="mt-5">Autres liens :</h5>
        <p><a href="https://www.linkedin.com/posts/datactive40_google-local-pack-activity-6990208845226594304-XTRK?utm_source=share&utm_medium=member_desktop" class="text-info" target="_blank">En savoir plus sur le Google Local Pack <i class="fa-solid fa-arrow-up-right-from-square"></i></a></p>
        <p><a href="https://www.data-transitionnumerique.com/selenium-python/" class="text-info" target="_blank">Qu'est ce que Selenium <i class="fa-solid fa-arrow-up-right-from-square"></i></a></p>
    </div>
{% endblock content %}